{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source dataset: oxford-iiit-pet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of dataset: 7390\n",
      "\n",
      "--num dataset != 200\n",
      "staffordshire_bull_terrier 191\n",
      "scottish_terrier 199\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "img_list = ['_'.join(i.split('_')[:-1]) for i in os.listdir('/media/disk1/Data/oxford-iiit-pet/images') if i.endswith('.jpg')]\n",
    "sets = set(img_list)\n",
    "\n",
    "print('total number of dataset:', len(img_list))\n",
    "\n",
    "print()\n",
    "print('--num dataset != 200')\n",
    "\n",
    "for name in sets:\n",
    "    if img_list.count(name) != 200:\n",
    "        print(name, len([i for i in img_list if i == name]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpurioudCatDog dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpuriousCatDogVer4 dataset is already prepared\n"
     ]
    }
   ],
   "source": [
    "from prepare_dataset import PrepareDataset\n",
    "\n",
    "prep = PrepareDataset(\n",
    "    places365_root='/media/disk2/Data',\n",
    "    save_root='/media/disk1/Data',\n",
    "    oxford_iiit_pet_root='/media/disk1/Data',\n",
    "    dataset=\"SpuriousCatDogVer4\", \n",
    "    download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jj/anaconda3/envs/torch2.1_cuda11.8/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/jj/anaconda3/envs/torch2.1_cuda11.8/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataModule\n\u001b[1;32m      3\u001b[0m dm \u001b[38;5;241m=\u001b[39m DataModule(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpuriousCatDogVer3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/media/disk1/Data\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1234\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/Research/ConceptualSensitivityRegularization/csr/module/dataset/data_module.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Union, List\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# from .parser import str2bool\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     CelebA,\n\u001b[1;32m     16\u001b[0m     CelebAGender,\n\u001b[1;32m     17\u001b[0m     Waterbirds,\n\u001b[1;32m     18\u001b[0m     ColoredMNIST,\n\u001b[1;32m     19\u001b[0m     ConceptDataset,\n\u001b[1;32m     20\u001b[0m     Dogs,\n\u001b[1;32m     21\u001b[0m     CatDog,\n\u001b[1;32m     22\u001b[0m     CatDog2,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDataSubset\u001b[39;00m(Dataset):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28mself\u001b[39m, dataset: Dataset, idxs: Union[List, np\u001b[38;5;241m.\u001b[39mndarray], minor_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     ):\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from data_module import DataModule\n",
    "\n",
    "dm = DataModule('SpuriousCatDogVer3', '/media/disk1/Data', 1234, 2, 16, 16, None, None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spurious CatDog generation code debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# df = pd.read_csv('/home/data/waterbirds/waterbird_complete95_forest2water2/metadata.csv')\n",
    "# df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "root = '/media/disk1/Data/oxford-iiit-pet/'\n",
    "img_dir = os.path.join(root, 'images')\n",
    "mask_dir = os.path.join(root, 'annotations/trimaps')\n",
    "\n",
    "img_list = sorted([i for i in os.listdir(img_dir) if '.jpg' in i])\n",
    "\n",
    "df = pd.DataFrame(img_list, columns=['fg_filename'])\n",
    "\n",
    "df['species'] = df['fg_filename'].apply(lambda x: '_'.join(x.split('_')[:-1]))\n",
    "df['y'] = df['species'].apply(lambda x: 0 if x[0].isupper() else 1)\n",
    "\n",
    "# set 'split' and 'place' of df.\n",
    "# The correlation between 'place' and 'y' is 0.95 for training dataset\n",
    "# The correlation between 'place' and 'y' is 0.9 for validation dataset\n",
    "df['split'] = 'train'\n",
    "df['place'] = 0\n",
    "for species in df['species'].unique():\n",
    "    species_df = df[df['species'] == species]\n",
    "    test_idx = species_df.sample(frac=0.2, random_state=42).index\n",
    "    val_idx = species_df.drop(test_idx).sample(frac=0.2, random_state=42).index\n",
    "    train_idx = species_df.drop(test_idx).drop(val_idx).index\n",
    "    \n",
    "    df.loc[test_idx, 'split'] = 'test'\n",
    "    df.loc[val_idx, 'split'] = 'val'\n",
    "\n",
    "    # Set 'place'\n",
    "    label0_idx = df[df['y'] == 0].index\n",
    "    label1_idx = df[df['y'] == 1].index\n",
    "\n",
    "    df.loc[label0_idx.join(train_idx, how='inner'), 'place'] = np.random.choice([0, 1], size=len(label0_idx.join(train_idx, how='inner')), p=[0.95, 0.05])\n",
    "    df.loc[label1_idx.join(train_idx, how='inner'), 'place'] = np.random.choice([0, 1], size=len(label1_idx.join(train_idx, how='inner')), p=[0.05, 0.95])\n",
    "    df.loc[label0_idx.join(val_idx, how='inner'), 'place'] = np.random.choice([0, 1], size=len(label0_idx.join(val_idx, how='inner')), p=[0.5, 0.5])\n",
    "    df.loc[label1_idx.join(val_idx, how='inner'), 'place'] = np.random.choice([0, 1], size=len(label1_idx.join(val_idx, how='inner')), p=[0.5, 0.5])\n",
    "\n",
    "test_idx = df[df['split'] == 'test'].index\n",
    "df.loc[test_idx, 'place'] = 0\n",
    "test_df = df.loc[test_idx].copy(deep=True)\n",
    "test_df['place'] = 1\n",
    "\n",
    "df = pd.concat([df, test_df], axis=0)\n",
    "df = df.reset_index()\n",
    "\n",
    "# allocates place_filename to each row of df\n",
    "\n",
    "\n",
    "# load yaml file\n",
    "import yaml\n",
    "with open('/home/jj/Research/ConceptualSensitivityRegularization/configs/dataset/waterbirds_used_bgs.yaml', 'r') as f:\n",
    "    config_waterbirds_bg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "places_dict = {\n",
    "    0: config_waterbirds_bg['water'],\n",
    "    1: config_waterbirds_bg['land']\n",
    "}\n",
    "\n",
    "df.loc[df['place'] == 0, 'place_filename'] = places_dict[0][:sum(df['place'] == 0)]\n",
    "df.loc[df['place'] == 1, 'place_filename'] = places_dict[1][:sum(df['place'] == 1)]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['species', 'split', 'y', 'place']).count().pivot_table(index=['split', 'y', 'place'], columns=['species'], values='fg_filename', aggfunc='sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['split', 'y', 'place']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['split']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_resize(source_img, target_img):\n",
    "    \"\"\"\n",
    "    Make source_img exactly the same as target_img by expanding/shrinking and\n",
    "    cropping appropriately.\n",
    "\n",
    "    If source_img's dimensions are strictly greater than or equal to the\n",
    "    corresponding target img dimensions, we crop left/right or top/bottom\n",
    "    depending on aspect ratio, then shrink down.\n",
    "\n",
    "    If any of source img's dimensions are smaller than target img's dimensions,\n",
    "    we expand the source img and then crop accordingly\n",
    "\n",
    "    Modified from\n",
    "    https://stackoverflow.com/questions/4744372/reducing-the-width-height-of-an-image-to-fit-a-given-aspect-ratio-how-python\n",
    "    \"\"\"\n",
    "    source_width = source_img.size[0]\n",
    "    source_height = source_img.size[1]\n",
    "\n",
    "    target_width = target_img.size[0]\n",
    "    target_height = target_img.size[1]\n",
    "\n",
    "    # Check if source does not completely cover target\n",
    "    if (source_width < target_width) or (source_height < target_height):\n",
    "        # Try matching width\n",
    "        width_resize = (target_width, int((target_width / source_width) * source_height))\n",
    "        if (width_resize[0] >= target_width) and (width_resize[1] >= target_height):\n",
    "            source_resized = source_img.resize(width_resize, Image.ANTIALIAS)\n",
    "        else:\n",
    "            height_resize = (int((target_height / source_height) * source_width), target_height)\n",
    "            assert (height_resize[0] >= target_width) and (height_resize[1] >= target_height)\n",
    "            source_resized = source_img.resize(height_resize, Image.ANTIALIAS)\n",
    "        # Rerun the cropping\n",
    "        return crop_and_resize(source_resized, target_img)\n",
    "\n",
    "    source_aspect = source_width / source_height\n",
    "    target_aspect = target_width / target_height\n",
    "\n",
    "    if source_aspect > target_aspect:\n",
    "        # Crop left/right\n",
    "        new_source_width = int(target_aspect * source_height)\n",
    "        offset = (source_width - new_source_width) // 2\n",
    "        resize = (offset, 0, source_width - offset, source_height)\n",
    "    else:\n",
    "        # Crop top/bottom\n",
    "        new_source_height = int(source_width / target_aspect)\n",
    "        offset = (source_height - new_source_height) // 2\n",
    "        resize = (0, offset, source_width, source_height - offset)\n",
    "\n",
    "    source_resized = source_img.crop(resize).resize((target_width, target_height), Image.ANTIALIAS)\n",
    "    return source_resized\n",
    "\n",
    "\n",
    "def combine_and_mask(img_new, mask, img_black):\n",
    "    \"\"\"\n",
    "    Combine img_new, mask, and image_black based on the mask\n",
    "\n",
    "    img_new: new (unmasked image)\n",
    "    mask: binary mask of bird image\n",
    "    img_black: already-masked bird image (bird only)\n",
    "    \"\"\"\n",
    "    # Warp new img to match black img\n",
    "    img_resized = crop_and_resize(img_new, img_black)\n",
    "    img_resized_np = np.asarray(img_resized)\n",
    "\n",
    "    # Mask new img\n",
    "    img_masked_np = np.around(img_resized_np * (1 - mask)).astype(np.uint8)\n",
    "\n",
    "    # Combine\n",
    "    img_combined_np = np.asarray(img_black) + img_masked_np\n",
    "    img_combined = Image.fromarray(img_combined_np)\n",
    "\n",
    "    return img_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_pad(img, ratio):\n",
    "    \"\"\"\n",
    "    Resize img to ratio and pad with black at center of the original image\n",
    "    \"\"\"\n",
    "    width = img.size[0]\n",
    "    height = img.size[1]\n",
    "    img_resized = img.resize((int(width * ratio), int(height * ratio)), Image.ANTIALIAS)\n",
    "\n",
    "    # Pad with black\n",
    "    img_padded = Image.new('RGB', (width, height), (0, 0, 0))\n",
    "    img_padded.paste(img_resized, ((width - img_resized.size[0]) // 2, (height - img_resized.size[1]) // 2))\n",
    "\n",
    "    return img_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "root = '/media/disk1/Data/'\n",
    "fg_dir = os.path.join(root, 'oxford-iiit-pet/images')\n",
    "mask_dir = os.path.join(root, 'oxford-iiit-pet/annotations/trimaps')\n",
    "bg_dir = os.path.join(root, 'Places205/data/vision/torralba/deeplearning/images256/')\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    fg_fname = row['fg_filename']\n",
    "    bg_fname = row['place_filename']\n",
    "\n",
    "    fg_path = os.path.join(fg_dir, fg_fname)\n",
    "    bg_path = os.path.join(bg_dir, bg_fname)\n",
    "    mask_path = os.path.join(mask_dir, fg_fname.replace('.jpg', '.png'))\n",
    "\n",
    "    fg = Image.open(fg_path).convert('RGB')\n",
    "    mask = Image.open(mask_path).convert('RGB')\n",
    "    bg = Image.open(bg_path).convert('RGB')\n",
    "\n",
    "    fg_np =  np.asarray(resize_and_pad(fg, 0.7))\n",
    "    mask_np = np.float32(np.asarray(resize_and_pad(mask, 0.7)) == 1)\n",
    "\n",
    "    fg_only = Image.fromarray(np.around(fg_np * mask_np).astype(np.uint8))\n",
    "    combined_img = combine_and_mask(bg, mask_np, fg_only)\n",
    "\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_pad(img, ratio):\n",
    "    \"\"\"\n",
    "    Resize img to ratio and pad with black at center of the original image\n",
    "    \"\"\"\n",
    "    width = img.size[0]\n",
    "    height = img.size[1]\n",
    "    img_resized = img.resize((int(width * ratio), int(height * ratio)), Image.ANTIALIAS)\n",
    "\n",
    "    # Pad with black\n",
    "    img_padded = Image.new('RGB', (width, height), (0, 0, 0))\n",
    "    img_padded.paste(img_resized, ((width - img_resized.size[0]) // 2, (height - img_resized.size[1]) // 2))\n",
    "\n",
    "    return img_padded\n",
    "\n",
    "resize_and_pad(fg, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_filename = f\"{row['fg_filename'].split('.')[0]}_{row['place_filename'].split('/')[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_save_path = os.path.join(root, '/home/data/SpuriousCatDogVer3')\n",
    "os.path.join(root, \"SpuriousCatDogVer3\", dataset_save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lit2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
